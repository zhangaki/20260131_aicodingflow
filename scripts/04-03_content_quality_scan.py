"""
å…¨é‡å†…å®¹è´¨é‡æ‰«æå™¨

æ‰«ææ‰€æœ‰åšå®¢æ–‡ç« ï¼Œæ£€æµ‹ï¼š
1. è·¨æ–‡ç« é‡å¤å¥å­ï¼ˆæ¨¡æ¿å¤åˆ¶ç²˜è´´ï¼‰
2. è™šæ„å®šä»·/ç»Ÿè®¡æ•°æ®
3. AI æ¨¡æ¿å¡«å……è¯­å¥
4. è¿‡æ—¶çš„æ¨¡å‹/ç‰ˆæœ¬å¼•ç”¨

è¾“å‡ºæ¯ç¯‡æ–‡ç« çš„é£é™©è¯„åˆ†ï¼ŒæŒ‰ä¸¥é‡ç¨‹åº¦æ’åºã€‚

Usage:
    python content_quality_scan.py              # æ‰«æå¹¶è¾“å‡ºæŠ¥å‘Š
    python content_quality_scan.py --export     # å¯¼å‡º CSV
"""

import csv
import json
import re
import sys
from collections import Counter, defaultdict
from pathlib import Path

PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent.parent
BLOG_DIR = PROJECT_ROOT / "projects" / "20260131_seo-site" / "src" / "content" / "blog"

# ============================================================
# æ£€æµ‹è§„åˆ™
# ============================================================

# AI æ¨¡æ¿å¡«å……å¥ï¼ˆç²¾ç¡®åŒ¹é…è¿™äº›æ¨¡å¼è¯´æ˜æ˜¯æ¨¡æ¿ç”Ÿæˆï¼‰
TEMPLATE_PATTERNS = [
    r"has carved out a strong position",
    r"in our internal production environment",
    r"the feedback has been overwhelmingly positive",
    r"it's been a mainstay in our stack ever since",
    r"we first discovered .+ during a hackathon",
    r"always verify ai-generated output before using",
    r"alone can justify the subscription cost",
    r"delivers tangible productivity improvements",
    r"genuinely useful in daily workflows",
    r"features that seemed simple on the surface had surprising power",
    r"nobody on the team wanted to switch back",
    r"in today'?s rapidly evolving",
    r"in the (ever-evolving|rapidly changing|fast-paced) (world|landscape) of",
    r"let'?s (dive|delve) (in|into|deeper)",
    r"without further ado",
    r"game.?changer for",
    r"revolutioniz(e|ing|ed) the way",
    r"paradigm shift",
    r"stands out as a (beacon|leader|pioneer)",
    r"whether you'?re a .+ or a .+, this",
]

# è™šæ„æ•°æ®æ¨¡å¼
FAKE_DATA_PATTERNS = [
    (r"\b\d+% (success|accuracy|improvement|reduction|increase) rate\b", "unverified percentage claim"),
    (r"\bavg rating: \d\.\d/5\b", "fabricated rating"),
    (r"\b(based on|according to) (developer surveys|online reviews|our testing)\b", "vague source attribution"),
    (r"\bcatches .+ (\d+-\d+ hours|days) before\b", "fabricated prediction claim"),
]

# è¿‡æ—¶æ¨¡å‹å¼•ç”¨ï¼ˆ2026 å¹´æ–‡ç« ä¸åº”å¼•ç”¨è¿™äº›ä¸º"æœ€æ–°"ï¼‰
OUTDATED_REFS = [
    (r"claude 3\.5 sonnet.{0,30}(best|top|leading|recommended)", "Claude 3.5 Sonnet listed as current best"),
    (r"grok 2\b(?!.*grok 3)", "References Grok 2 without mentioning Grok 3"),
    (r"gpt-4o?\b(?!.*gpt-4\.\d|.*o1|.*o3)", "References GPT-4 without newer models"),
    (r"model-as-code platform", "Wrong MCP definition (should be Model Context Protocol)"),
]


def extract_body(content):
    """å»æ‰ frontmatterï¼Œåªç•™æ­£æ–‡"""
    if content.startswith("---"):
        parts = content.split("---", 2)
        if len(parts) >= 3:
            return parts[2]
    return content


def extract_sentences(text):
    """æå–æ‰€æœ‰æœ‰æ„ä¹‰çš„å¥å­ï¼ˆ>= 10 è¯ï¼‰"""
    # å»æ‰ markdown è¯­æ³•
    text = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', text)  # links
    text = re.sub(r'[#*>`\-]', ' ', text)  # markdown
    text = re.sub(r'\s+', ' ', text)

    sentences = re.split(r'[.!?]\s+', text)
    return [s.strip().lower() for s in sentences if len(s.split()) >= 10]


def scan_article(filepath, all_sentences_index):
    """æ‰«æå•ç¯‡æ–‡ç« ï¼Œè¿”å›é—®é¢˜åˆ—è¡¨å’Œåˆ†æ•°"""
    content = filepath.read_text(encoding="utf-8")
    body = extract_body(content)
    body_lower = body.lower()
    slug = filepath.stem

    issues = []
    score = 0  # è¶Šé«˜è¶Šå·®

    # 1. æ¨¡æ¿å¡«å……æ£€æµ‹
    template_hits = 0
    for pattern in TEMPLATE_PATTERNS:
        matches = re.findall(pattern, body_lower)
        if matches:
            template_hits += len(matches)
            issues.append(f"TEMPLATE: '{pattern}' matched {len(matches)}x")
    score += template_hits * 5

    # 2. è™šæ„æ•°æ®æ£€æµ‹
    fake_hits = 0
    for pattern, desc in FAKE_DATA_PATTERNS:
        matches = re.findall(pattern, body_lower)
        if matches:
            fake_hits += len(matches)
            issues.append(f"FAKE_DATA: {desc} ({len(matches)}x)")
    score += fake_hits * 3

    # 3. è¿‡æ—¶å¼•ç”¨æ£€æµ‹
    outdated_hits = 0
    for pattern, desc in OUTDATED_REFS:
        matches = re.findall(pattern, body_lower)
        if matches:
            outdated_hits += 1
            issues.append(f"OUTDATED: {desc}")
    score += outdated_hits * 4

    # 4. è·¨æ–‡ç« é‡å¤å¥å­æ£€æµ‹
    sentences = extract_sentences(body)
    dup_count = 0
    for sent in sentences:
        if sent in all_sentences_index and len(all_sentences_index[sent]) > 1:
            other_files = [f for f in all_sentences_index[sent] if f != slug]
            if other_files:
                dup_count += 1
    if dup_count > 0:
        issues.append(f"DUPLICATE: {dup_count} sentences found in other articles")
        score += dup_count * 2

    # 5. å†…å®¹ç©ºæ´åº¦ï¼ˆæ–‡ç« å¤ªçŸ­ or FAQ å æ¯”å¤ªé«˜ï¼‰
    word_count = len(body.split())
    if word_count < 800:
        issues.append(f"THIN: only {word_count} words")
        score += 10

    # 6. è™šæ„å®šä»·æ£€æµ‹ï¼ˆ$X/month æ¨¡å¼è¿‡å¤šï¼‰
    price_claims = re.findall(r'\$\d+(?:\.\d+)?/(?:month|mo|user|seat)', body_lower)
    if len(price_claims) > 5:
        issues.append(f"PRICING: {len(price_claims)} price claims (likely unverified)")
        score += len(price_claims)

    return {
        "slug": slug,
        "word_count": word_count,
        "score": score,
        "template_hits": template_hits,
        "fake_hits": fake_hits,
        "outdated_hits": outdated_hits,
        "dup_count": dup_count,
        "issues": issues,
    }


def build_sentence_index(articles):
    """å»ºç«‹è·¨æ–‡ç« å¥å­ç´¢å¼•ï¼Œç”¨äºæ£€æµ‹é‡å¤"""
    index = defaultdict(set)
    for filepath in articles:
        content = filepath.read_text(encoding="utf-8")
        body = extract_body(content)
        slug = filepath.stem
        for sent in extract_sentences(body):
            index[sent].add(slug)
    return index


def main():
    export = "--export" in sys.argv

    articles = sorted(BLOG_DIR.glob("*.md")) + sorted(BLOG_DIR.glob("*.mdx"))
    if not articles:
        print("æœªæ‰¾åˆ°æ–‡ç« ")
        return

    print(f"æ‰«æ {len(articles)} ç¯‡æ–‡ç« ...\n")

    # Phase 1: å»ºç«‹è·¨æ–‡ç« å¥å­ç´¢å¼•
    print("Phase 1: å»ºç«‹è·¨æ–‡ç« å¥å­ç´¢å¼•...")
    sentence_index = build_sentence_index(articles)

    # ç»Ÿè®¡é‡å¤å¥å­æ•°é‡
    dup_sentences = {s: files for s, files in sentence_index.items() if len(files) > 1}
    print(f"  å‘ç° {len(dup_sentences)} ä¸ªé‡å¤å¥å­ï¼ˆå‡ºç°åœ¨ 2+ ç¯‡æ–‡ç« ä¸­ï¼‰")

    # Phase 2: é€ç¯‡æ‰«æ
    print("Phase 2: é€ç¯‡æ‰«æ...")
    results = []
    for filepath in articles:
        result = scan_article(filepath, sentence_index)
        results.append(result)

    # æŒ‰é£é™©åˆ†æ’åº
    results.sort(key=lambda x: x["score"], reverse=True)

    # åˆ†çº§
    critical = [r for r in results if r["score"] >= 20]
    warning = [r for r in results if 10 <= r["score"] < 20]
    ok = [r for r in results if r["score"] < 10]

    # è¾“å‡ºæŠ¥å‘Š
    print("\n" + "=" * 70)
    print(f"  å†…å®¹è´¨é‡æ‰«ææŠ¥å‘Š")
    print("=" * 70)
    print(f"\n  æ€»è®¡: {len(articles)} ç¯‡æ–‡ç« ")
    print(f"  ğŸ”´ ä¸¥é‡ (score >= 20): {len(critical)} ç¯‡")
    print(f"  ğŸŸ¡ è­¦å‘Š (score 10-19): {len(warning)} ç¯‡")
    print(f"  ğŸŸ¢ æ­£å¸¸ (score < 10):  {len(ok)} ç¯‡")

    if critical:
        print(f"\n{'â”€' * 70}")
        print("  ğŸ”´ ä¸¥é‡é—®é¢˜æ–‡ç« ï¼ˆå»ºè®® noindex æˆ–é‡å†™ï¼‰:")
        print(f"{'â”€' * 70}")
        for r in critical:
            print(f"\n  [{r['score']:3d}] {r['slug']}")
            print(f"        {r['word_count']} è¯ | æ¨¡æ¿:{r['template_hits']} è™šæ„:{r['fake_hits']} è¿‡æ—¶:{r['outdated_hits']} é‡å¤å¥:{r['dup_count']}")
            for issue in r["issues"][:5]:
                print(f"        - {issue}")

    if warning:
        print(f"\n{'â”€' * 70}")
        print("  ğŸŸ¡ éœ€å…³æ³¨æ–‡ç« :")
        print(f"{'â”€' * 70}")
        for r in warning:
            print(f"\n  [{r['score']:3d}] {r['slug']}")
            print(f"        {r['word_count']} è¯ | æ¨¡æ¿:{r['template_hits']} è™šæ„:{r['fake_hits']} è¿‡æ—¶:{r['outdated_hits']} é‡å¤å¥:{r['dup_count']}")
            for issue in r["issues"][:3]:
                print(f"        - {issue}")

    # æœ€å¸¸è§çš„é‡å¤å¥å­ Top 10
    print(f"\n{'â”€' * 70}")
    print("  è·¨æ–‡ç« é‡å¤æœ€å¤šçš„å¥å­ (Top 10):")
    print(f"{'â”€' * 70}")
    sorted_dups = sorted(dup_sentences.items(), key=lambda x: len(x[1]), reverse=True)[:10]
    for sent, files in sorted_dups:
        print(f"\n  [{len(files)} ç¯‡] \"{sent[:80]}...\"")
        print(f"        å‡ºç°åœ¨: {', '.join(sorted(files)[:5])}")
        if len(files) > 5:
            print(f"        ... è¿˜æœ‰ {len(files) - 5} ç¯‡")

    # å¯¼å‡º
    if export:
        export_path = PROJECT_ROOT / "data" / "content_quality_scan.csv"
        with open(export_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=[
                "slug", "score", "word_count", "template_hits", "fake_hits",
                "outdated_hits", "dup_count", "issues"
            ])
            writer.writeheader()
            for r in results:
                row = {**r, "issues": " | ".join(r["issues"])}
                writer.writerow(row)
        print(f"\nğŸ“„ å·²å¯¼å‡º: {export_path}")

    print()


if __name__ == "__main__":
    main()
